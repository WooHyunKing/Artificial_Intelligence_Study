{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_study1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOhuh1YcPz52jchmNUnq8+i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WooHyunKing/Artificial_Intelligence_Study/blob/main/AI_study1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dc0jNMWl6njt"
      },
      "outputs": [],
      "source": [
        "def Perceptron(x1,x2,w1,w2,theta):\n",
        "  y=w1*x1+w2*x2\n",
        "\n",
        "  if y<=theta:\n",
        "    return 0\n",
        "  elif y>theta:\n",
        "    return 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Perceptron(x1=0,x2=1,w1=0.1,w2=0.3,theta=0.6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyBNrqZb7zTA",
        "outputId": "78ea443a-ea90-4cbb-d31c-256129efe8c1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w1,w2,theta=0.5,0.5,0.8\n",
        "\n",
        "print(\"AND Gate\")\n",
        "print(\"----\"*20)\n",
        "print(\"(0,0):\",Perceptron(x1=0,x2=0,w1=w1,w2=w2,theta=theta))\n",
        "print(\"(0,1):\",Perceptron(x1=0,x2=1,w1=w1,w2=w2,theta=theta))\n",
        "print(\"(1,0):\",Perceptron(x1=1,x2=0,w1=w1,w2=w2,theta=theta))\n",
        "print(\"(1,1):\",Perceptron(x1=1,x2=1,w1=w1,w2=w2,theta=theta))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dttK-QZ68z67",
        "outputId": "be91441b-0724-4dc8-e248-57a7d290a699"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AND Gate\n",
            "--------------------------------------------------------------------------------\n",
            "(0,0): 0\n",
            "(0,1): 0\n",
            "(1,0): 0\n",
            "(1,1): 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w1,w2,theta=-0.5,-0.5,-0.8\n",
        "\n",
        "print(\"NAND Gate\")\n",
        "print(\"----\"*20)\n",
        "print(\"(0,0):\",Perceptron(x1=0,x2=0,w1=w1,w2=w2,theta=theta))\n",
        "print(\"(0,1):\",Perceptron(x1=0,x2=1,w1=w1,w2=w2,theta=theta))\n",
        "print(\"(1,0):\",Perceptron(x1=1,x2=0,w1=w1,w2=w2,theta=theta))\n",
        "print(\"(1,1):\",Perceptron(x1=1,x2=1,w1=w1,w2=w2,theta=theta))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc9c44c4-d9b2-420f-a776-4d9b1fcd7473",
        "id": "f1jGonY59ZMc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAND Gate\n",
            "--------------------------------------------------------------------------------\n",
            "(0,0): 1\n",
            "(0,1): 1\n",
            "(1,0): 1\n",
            "(1,1): 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w1,w2,theta=0.5,0.5,0.2\n",
        "\n",
        "print(\"OR Gate\")\n",
        "print(\"----\"*20)\n",
        "print(\"(0,0):\",Perceptron(x1=0,x2=0,w1=w1,w2=w2,theta=theta))\n",
        "print(\"(0,1):\",Perceptron(x1=0,x2=1,w1=w1,w2=w2,theta=theta))\n",
        "print(\"(1,0):\",Perceptron(x1=1,x2=0,w1=w1,w2=w2,theta=theta))\n",
        "print(\"(1,1):\",Perceptron(x1=1,x2=1,w1=w1,w2=w2,theta=theta))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d3eb68d-b163-4cad-bac2-47191e3243b6",
        "id": "lhqyRo519pIJ"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OR Gate\n",
            "--------------------------------------------------------------------------------\n",
            "(0,0): 0\n",
            "(0,1): 1\n",
            "(1,0): 1\n",
            "(1,1): 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Logical_gate:\n",
        "  def __init__(self,weight_dict):\n",
        "    self.weight=weight_dict\n",
        "  \n",
        "  def Perceptron(self,x1,x2,key):\n",
        "    weight=self.weight[key]\n",
        "    w=weight[\"w\"]\n",
        "    b=weight[\"b\"]\n",
        "    \n",
        "    y=w[0]*x1+w[1]*x2+b\n",
        "\n",
        "    if y<=0:\n",
        "      return 0\n",
        "    elif y>0:\n",
        "      return 1\n",
        "\n",
        "  def Run_Gate(self,key):\n",
        "    print(key + \" Gate\")\n",
        "    print(\"----\"*20)\n",
        "    print(\"(0, 0):\", self.Perceptron(0, 0, key))\n",
        "    print(\"(0, 1):\", self.Perceptron(0, 1, key))\n",
        "    print(\"(1, 0):\", self.Perceptron(1, 0, key))\n",
        "    print(\"(1, 1):\", self.Perceptron(1, 1, key))\n",
        "    print(\"----\"*20)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "sTwRtkud_MDz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_dict = {\"AND\":{\"w\":[0.5,0.5], \"b\":-0.5},\n",
        "                    \"NAND\":{\"w\":[-0.5,-0.5], \"b\":0.5},\n",
        "                     \"OR\":{\"w\":[0.5,0.5], \"b\":-0.2}}\n",
        "\n",
        "LG = Logical_gate(weight_dict)\n",
        "\n",
        "LG.Run_Gate(\"AND\")\n",
        "LG.Run_Gate(\"NAND\")\n",
        "LG.Run_Gate(\"OR\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeUXCTGrAsr5",
        "outputId": "5452b8d6-6b1d-4358-85a0-6820e300f4e0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AND Gate\n",
            "--------------------------------------------------------------------------------\n",
            "(0, 0): 0\n",
            "(0, 1): 0\n",
            "(1, 0): 0\n",
            "(1, 1): 1\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "NAND Gate\n",
            "--------------------------------------------------------------------------------\n",
            "(0, 0): 1\n",
            "(0, 1): 0\n",
            "(1, 0): 0\n",
            "(1, 1): 0\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "OR Gate\n",
            "--------------------------------------------------------------------------------\n",
            "(0, 0): 0\n",
            "(0, 1): 1\n",
            "(1, 0): 1\n",
            "(1, 1): 1\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def step_function1(x):\n",
        "  if x<=0:\n",
        "    return 0\n",
        "  else x>0:\n",
        "    return 1\n",
        "\n",
        "def step_function2(x):\n",
        "  y=x>0\n",
        "  return y.astype(np.int)\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "#장점 : Sigmoid는 출력 값의 범위가 0~1 사이이며, 매우 매끄러운 곡선을 가지므로, 경사하강법을 시행할 때\n",
        "#기울기가 급격하게 변해서 발산하는 기울기 폭주(Gradient Exploding)가 발생하지 않는다.\n",
        "#분류는 0과 1로 나뉘며, 출력 값이 어느 값에 가까운지를 통해 어느 분류에 속하는지 쉽게 알 수 있다.\n",
        "\n",
        "#단점 : 입력값이 아무리 크더라도, 출력되는 값의 범위가 매우 좁기 때문에 경사하강법 수행 시에 범위가 너무 좁아\n",
        "#0에 수렴하는 기울기 소실(Gradient Vanishing)이 발생할 수 있다.\n",
        "#시그모이드 함수의 출력값은 모두 양수기 때문에 경사하강법을 진행할 때, 그 기울기가 모두 양수거나\n",
        "#음수가 된다. 이는 기울기 업데이트가 지그재그로 변동하는 결과를 가지고 오고, 학습 효율성을\n",
        "#감소시켜 학습에 더 많은 시간이 들어가게 만든다.->보통 '출력층'에서만 사용됨"
      ],
      "metadata": {
        "id": "PBe476TTSlwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  array_x=x-np.max(x)\n",
        "  exp_x=np.exp(array_x)\n",
        "  result=exp_x/np.sum(exp_x)\n",
        "\n",
        "  return result\n",
        "\n",
        "#이진 분류 활성화 함수인 sigmoid가 아닌 다중 분류에 주로 사용되는 활성화 함수 : Softmax 함수\n",
        "#Softmax는 세 개 이상으로 분류하는 다중 클래스 분류에서 사용되는 활성화 함수로 분류될 클래스가\n",
        "#n개라고 할 때, n차원의 벡터를 입력받아서 각 클래스에 속할 \"확률\"을 추정한다.\n",
        "#sigmoid 함수처럼 출력층에서 주로 사용되며 이진 분류에서만 사용되는 sigmoid 함수와는 달리 다중\n",
        "#분류에서 주로 사용된다.\n",
        "#가장 큰 장점은 확률의 총합이 1이므로, 어떤 분류에 속할 확률이 가장 높을지를 쉽게 인지가능"
      ],
      "metadata": {
        "id": "YJXA94wiYoKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(x):\n",
        "  p_exp_x=np.exp(x)\n",
        "  m_exp_x=np.exp(-x)\n",
        "  y=(p_exp_x-m_exp_x)/(p_exp_x+m_exp_x)\n",
        "\n",
        "  return y\n",
        "\n",
        "#tanh함수와 sigmoid함수의 가장 큰 차이점은 '출력값의 범위'로, tanh함수는 -1에서 1사이의 값을 출력하며 중앙값도 0이다\n",
        "#tanh함수는 중앙값이 0이기 때문에, 경사하강법 사용 시 sigmoid함수에서 발생하는 편향 이동이 발생하지 않는다.\n",
        "#즉, 기울기가 양수 음수 모두 나올 수 있기 때문에 sigmoid함수보다 학습 효율성이 뛰어나다.\n",
        "#또한, sigmoid함수보다 범위가 넓기 때문에 출력값의 변화폭이 더 크고, 그로 인해 기울기 소실(Gradient Vanishing)이 더 적은 편이다.\n",
        "#때문에 은닉층에서 sigmoid함수와 같은 역할을 레이어를 쌓고자 한다면, tanh함수를 사용하는 것이 효과적이다.\n",
        "#그러나 tanh함수 또한 x가 -5보다 작고 5보다 큰 경우, 기울기가 0으로 작아져 소실되는 기울기 소실 현상 문제는 여전히 존재한다.\n"
      ],
      "metadata": {
        "id": "z8IyIU7JaXP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ReLU(x):\n",
        "  return np.maximum(0,x)\n",
        "\n",
        "#ReLU함수는 +신호는 그대로, -신호는 차단하는 함수이다. 말 그대로 양수면 자기 자신을 반환하고 음수면 0을 반환한다.\n",
        "#ReLU함수는 양수는 그대로, 음수는 0으로 반환하다보니 특정 양수 값에 수렴하지 않는다.\n",
        "#즉, 출력값의 범위가 넓고, 양수인 경우 자기 자신을 그대로 반환하기 때문에 심층 신견망인 딥러닝에서\n",
        "#sigmoid함수를 활성화 함수로 사용해 발생한 문제였던 기울기 소실(Gradient Vanishing)문제가 발생하지 않는다.\n",
        "#단순한 공식을 가지고 있다보니 경사 하강시 다른 활성화 함수에 비해 학습 속도가 매우 빠르다.\n",
        "#가중치가 업데이트 되는 과정에서 가중치 합이 음수가 되는 순간 ReLU는 0을 반환하기 때문에 해당 뉴런은\n",
        "#그 이후로 0만 반환하는 아무것도 변하지 않는 현상이 발생할 수 있다.\n",
        "#이러한 죽은 뉴런을 초래하는 현상을 죽어가는 렐루(Dying ReLU)현상이라고 한다.\n",
        "#또한 ReLU함수는 기울기 소실 문제 방지를 위해 사용하는 활성화 함수이기 때문에 은닉층에서만 사용하는 것을 추천.\n",
        "#ReLU의 출력값은 0 또는 양수이며 기울기도 0 또는 1이므로 둘다 양수이다. 이로 인해 sigmoid함수처럼 가중치 업데이트시\n",
        "#지그제그로 최적의 가중치를 찾아가는 지그재그 현상이 발생한다.\n",
        "#또한, ReLU는 0에서 미분이 불가능하다(이에 대해 활성화함수는 미분 불가능하다 할지라도 출력값 문제는 아니고 0에 걸릴 확률이 적어서 무시하고 사용)"
      ],
      "metadata": {
        "id": "zIhVE8Jib8RD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Leaky_ReLU(x):\n",
        "  return np.maximum(0.01*x,x)\n",
        "\n",
        "def PReLU(x,a):\n",
        "  return np.maximum(a*x,x)"
      ],
      "metadata": {
        "id": "i_WzkhiXf9OV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}