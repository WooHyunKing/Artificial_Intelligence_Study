{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_study2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPckSecUIypkqUa/IkAXnmW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WooHyunKing/Artificial_Intelligence_Study/blob/main/AI_study2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7R6xuervtKw",
        "outputId": "16b3bc65-9888-42aa-d520-3e77e8c6ec65"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9241418199787566"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "x=np.array([2,5,1])\n",
        "w=np.array([0.4,0.2,0.7])\n",
        "Z=np.sum(x*w)\n",
        "sigmoid(Z)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.array([1,3,5])\n",
        "B=np.array([0.2,0.3])\n",
        "W=np.array([[0.3,0.5],[0.4,0.2],[0.7,0.3]])\n",
        "\n",
        "print(\"X shape:\",X.shape)\n",
        "print(\"B shape:\",B.shape)\n",
        "print(\"W shape\",W.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Qb23f353HBx",
        "outputId": "7345d977-35af-46da-e693-12fbae536822"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (3,)\n",
            "B shape: (2,)\n",
            "W shape (3, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Z=np.dot(X,W)+B\n",
        "Z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgZf-8FS36YP",
        "outputId": "ef55ba41-9f46-4fbe-ab9d-a7d3ec02527b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.2, 2.9])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sigmoid(Z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nijiGt2S4eJL",
        "outputId": "3bd6b091-615c-451c-a760-64036fe95963"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.9945137 , 0.94784644])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "활성화 함수에 정보를 전달하느냐 마느냐인 계단 함수를 넣는 것이 아니라 sigmoid, softmax, tanh, relu 등 다양한 활성화 함수를 넣고 단층 퍼셉트론이 아닌, 다층 퍼셉트론을 만들고 가중치를 인간이 수동으로 만드는 것이 아닌, 자동으로 가장 적합한 값을 찾아내는 것이 바로 인공 신경망이다.\n",
        "\n",
        "입력층(Input layer) : 학습 데이터셋(Train dataset)이 입력되는 곳이다. **학습 데이터의 Feature의 차원 수 만큼의 뉴런 개수를 가진다. **입력층은 단 한층만 존재한다.\n",
        "\n",
        "은닉층(Hidden layer) : 입력과 출력층 사이의 모든 층이다. 은닉층이라고 불리는 이유는 입력층과 출력층은 input되는 dataset과 output된 dataset을 눈으로 확인할 수 있지만, 은닉층은 보이지 않기 때문이다.\n",
        "\n",
        "출력층(Output layer) : 출력하고자 하는 데이터의 형태에 따라 노드의 수가 바뀐다.예를 들어서 0부터 9까지 열 종류의 숫자가 있다고 할 때, 이를 분류하고자 하면 출력층의 노드 수는 10개가 되며, sigmoid함수를 사용하여 이진 분류를 하고자 하는 경우에는 노드의 수가 1개가 된다."
      ],
      "metadata": {
        "id": "0vNmOXY26EK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([10, 2])\n",
        "\n",
        "W1 = np.array([[0.1, 0.3, 0.5], [0.6, 0.4, 0.2]])\n",
        "W2 = np.array([[0.1, 0.2],[0.2, 0.4],[0.3, 0.6]])\n",
        "W3 = np.array([[0.2, 0.4],[0.4, 0.8]])\n",
        "\n",
        "B1 = np.array([0.7, 0.6, 0.5])\n",
        "B2 = np.array([0.6, 0.4])\n",
        "B3 = np.array([0.5, 0.6])\n",
        "\n",
        "print(\"X Shape:\", X.shape)\n",
        "print(\"----\"*20)\n",
        "print(\"W1 Shape:\", W1.shape)\n",
        "print(\"W2 Shape:\", W2.shape)\n",
        "print(\"W3 Shape:\", W3.shape)\n",
        "print(\"----\"*20)\n",
        "print(\"B1 Shape:\", B1.shape)\n",
        "print(\"B2 Shape:\", B2.shape)\n",
        "print(\"B3 Shape:\", B3.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpsFcOJG70Rr",
        "outputId": "8997e920-90ab-4b6d-baec-65a7caf0352e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X Shape: (2,)\n",
            "--------------------------------------------------------------------------------\n",
            "W1 Shape: (2, 3)\n",
            "W2 Shape: (3, 2)\n",
            "W3 Shape: (2, 2)\n",
            "--------------------------------------------------------------------------------\n",
            "B1 Shape: (3,)\n",
            "B2 Shape: (2,)\n",
            "B3 Shape: (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ReLU(x):\n",
        "  return np.maximum(0,x)\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))"
      ],
      "metadata": {
        "id": "3AHpk1Rb8gqm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A1=np.dot(X,W1)+B1\n",
        "Z1=ReLU(A1)\n",
        "print(Z1)\n",
        "\n",
        "A2=np.dot(Z1,W2)+B2\n",
        "Z2=ReLU(A2)\n",
        "print(Z2)\n",
        "\n",
        "A3=np.dot(Z2,W3)+B3\n",
        "Y=sigmoid(A3)\n",
        "print(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0hg7r-U8yPZ",
        "outputId": "5620c709-989c-43a0-ac82-89caf0642fa4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.9 4.4 5.9]\n",
            "[3.54 6.28]\n",
            "[0.97633942 0.99912464]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 코드를 보면, 입력층 X에서 출발한 데이터가 \"행렬곱 > 활성화 함수 > 행렬곱 > 활성화 함수 > 행렬곱 > 활성화 함수 > 출력\"의 형태로 진행된 것을 알 수 있다.\n",
        "\n",
        "이를 보면, 단층 퍼셉트론을 활성화 함수만 바꾸면서 층을 쌓듯 여러 번 수행된 것을 알 수 있다.\n",
        "\n",
        "순전파(Forward Propagation):\n",
        "위 신경망에서 데이터의 흐름은 입력층에서 출력층의 방향으로 전달되었는데, 이를 순전파라고 한다.\n",
        "\n",
        "위에서 인공 신경망은 스스로 가장 적합한 가중치를 찾아간다고 하였는데, 이를 우리는 학습이라고 하며, 이 학습은 역전파(Back Propagation) 과정을 통해 이루어진다.\n",
        "\n",
        "만약 내가 신경망의 틀을 만들고, 그 신경망에서 가장 적합한 가중치를 찾는 것을, 이는 학습을 한다고 하며, 모델을 만든다라고 한다. \n",
        "지금 같이 이미 가중치를 알고 있는 상태는 학습이 끝난, 모델이 완성된 상태이며, 이 모델에 데이터를 집어넣어, 그 결과를 확인하는 것은 순전파 되어 실행된다."
      ],
      "metadata": {
        "id": "t9d3Y6f_-YQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 신경망 학습:\n",
        "이전 포스트에서 다층 퍼셉트론에서 데이터가 흐르는 것에 대해 학습해보았고, 그 과정에서 석연치 않은 부분이 하나 있었다.\n",
        "\n",
        " 바로 가중치가 이미 주어졌다는 것인데, 가중치를 저렇게 속 편하게 알고 있는 경우는 있을 수가 없으며, 그렇다고 가중치를 하나하나 찾아내는 것은 불가능에 가깝다.\n",
        "\n",
        " 한 층의 노드 수는 입력되는 텐서의 크기이기 때문에 한층에 수백 수 천 개에 달하는 노드가 존재할 수 있으며, 그러한 층이 무수히 많이 쌓이게 된다면, 각 노드에서 다음 층의 노드로 연결되는 가중치 엣지의 수가 셀 수 없이 많아지므로, 일일이 이를 구해 입력된 데이터가 내가 전혀 알지 못하는 분류대로 나눠지게 만드는 것이 가능할 리가 없다.\n",
        "\n",
        " 애초에 '딥러닝'이라는 기술은 ***엄청난 양의 데이터만 있고 거기에 숨겨진 함수 즉, 규칙을 모를 때 사용하는 것이며, 이 데이터 속에 막연한 현상이 숨어있을 것이라 추측하고 있는 상황에서, 어떻게 그 규칙을 찾아낼지도 모르고, 수많은 이론을 조합해 만들어낸 알고리즘이 정확할지도 모르기 때문에 사용하는 것***이다.\n",
        "\n",
        " 즉, <u>딥러닝은 순수하게 데이터만 가지고, 내가 분류하고자 하는 바에 가장 적합한 레이어를 쌓아 만들어낸 머신러닝 알고리즘에 데이터를 학습시켜, 최적의 가중치를 알아서 찾아내 모델을 만들어내고, 여기에 새로운 데이터들을 넣어 분류하는 것</u>이다. 때문에 딥러닝을 데이터 주도 학습이라고도 한다.\n",
        "\n",
        " 그렇다면, 어떻게 최적의 가중치를 찾을 수 있을까?\n",
        "\n",
        "1. 손실 함수(Loss Function):\n",
        "\n",
        "예를 들어서 우리가 1억 장에 달하는 고양이와 강아지 사진이 있다고 했을 때 사진의 양이 많기 때문에 일일이 강아지와 고양이로 구분하는 것은 불가능하다. \n",
        "\n",
        " 그래서 우리는 만 장의 사진에 대해 고양이는 0, 강아지는 1이라고 라벨(Label)을 붙였고(실제 값), 컴퓨터가 사진에서 찾아낸 특징을 기반으로 분류해낸 것(예측 값)의 차이가 작다면, 최적의 가중치를 찾았다고 할 수 있다.\n",
        "\n",
        " 바로 이 <u>실제 값과 예측 값의 오차</u>가 '손실 함수'이다.\n",
        "\n",
        " 오차가 클수록 손실 함수의 값이 커지고, 오차가 작아질수록 손실 함수의 값이 작아진다.\n",
        "\n",
        " 즉, 이 손실 함수가 0에 가깝게 줄어들게 만드는 것이 학습의 목표라고 할 수 있다.\n",
        "\n",
        " 손실함수는 이 오차를 비용이라고 판단하여 **비용 함수(Cost Function)**이라고도 한다.\n",
        "\n",
        "2. 최적화(Optimizer):\n",
        "\n",
        "우리는 이제 손실 함수를 이용해서 최적의 가중치를 찾을 수 있음을 알았는데, 그렇다면 어떻게 최적의 가중치를 찾아갈 수 있을까?\n",
        "\n",
        "먼저, 각 층에 임의의 가중치를 설정한다.(보통 가중치는 0, 편향은 1로 설정)\n",
        "\n",
        "<u>학습 데이터셋을 모델에 통과시켜서 출력값을 계산하고 출력값과 실제값이 허용 오차 이내가 되도록 각 층의 가중치를 Update한다.</u>\n",
        "\n",
        "이 과정에서 출력값과 실제값의 차이를 나타내는 지표로 사용되는 것이 **'손실 함수'**이다.\n",
        "\n",
        "<u>손실함수를 최소화시키기 위해, 가중치의 미분(기울기)을 계산하고 그 미분값을 기반으로 가장 적합한 가중치 값을 갱신하는 과정을 반복한다.</u>\n",
        "\n",
        "이 '기울기를 기반으로 최적의 미분 값을 찾아가는 방식'을 **최적화(Optimizer)**라고 하며, 그 유명한 경사하강법(Gradient Descent)이 여기에 해당한다.\n",
        "\n",
        "참고로 손실함수와 유사한 정확도(Accuracy)라는 것이 있는데 손실함수는 연속적으로 변해서 미분 가능하지만 정확도는 가중치의 변화에 둔감하고, 불연속적으로 변해 미분이 불가능하며, 손실함수를 지표로 학습을 해나간다.\n",
        "\n",
        "3. 역전파(Back Propagation):\n",
        "\n",
        "이제 우리는 최적화를 통해서 최적의 가중치를 찾을 수 있다. 그렇다면 어떻게 이것을 모델에 반영해줄 것인가?\n",
        "\n",
        "역전파는 최적화를 효율적으로 할 수 있게 해주는 알고리즘으로, <u>순전파와 반대방향으로 실제값과 예측값의 오차를 전파하여, 가중치를 업데이트하고 최적의 학습 결과를 찾아간다.</u>\n",
        "\n",
        "먼저 Foward propagation을 통해 출력층에서 오차를 계산하고 이를 다시 입력층으로 Back propagation시켜 가중치를 업데이트하고, 다시 입력값을 넣어 새로운 오차를 계산하고 이를 또 Back propagation시켜서 가중치를 업데이트하는 것을 반복한다.\n",
        "\n",
        "즉, **\"순전파>역전파>가중치 업데이트>순전파>역전파>가중치 업데이트 ..\"**의 과정으로 학습이 이루어진다.\n",
        "\n",
        "4. 정리:\n",
        "\n",
        "손실 함수(Loss function) : 가중치가 얼마나 잘 만들어 졌는지를 확인하는 방법\n",
        "\n",
        "최적화(Optimizer) : 손실함수를 기반으로 최적의 가중치를 찾는 방법\n",
        "\n",
        "역전파(Back propagation) : 가중치를 효율적으로 업데이트하는 방법\n",
        "\n",
        "이 3가지 방법이 서로 앙상블을 이루어 신경망에서 가장 적합한 가중치를 찾아낸다."
      ],
      "metadata": {
        "id": "DWToLhSrBjXe"
      }
    }
  ]
}