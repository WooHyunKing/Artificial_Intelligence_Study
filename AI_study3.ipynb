{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI study3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zX_9fiFNCQ0r"
      ],
      "authorship_tag": "ABX9TyPi9/DqjmaIZDoFuus+UDQA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WooHyunKing/Artificial_Intelligence_Study/blob/main/AI_study3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "지금까지 가중치를 평가하는 방법인 손실 함수에 대해 학습해보았는데, 그렇다면 어떻게 손실 함수를 기반으로 최적의 가중치를 찾아낼까?\n",
        "\n",
        "이번에는 손실 함수로부터 어떻게 경사하강법이 나오게 되었는지를 이야기해보고, 경사하강법을 위주로 배워보기로 한다."
      ],
      "metadata": {
        "id": "29SuNCtICAGK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 손실함수와 경사하강법\n",
        "\n",
        "1. 최적화(Optimizer):\n",
        "\n",
        "최적화란 손실함수의 결과값을 최소화하는 가중치를 찾아내는 것이 목적이다.\n",
        "\n",
        "예상한 값과 실제값의 차이인 제곱 오차(SE)를 가지고, 손실함수의 개형을 알아보자.\n",
        "\n",
        "SE = (실제값 - 예측값)^2\n",
        "\n",
        "이해하기 쉽도록 예측값을 변수(가중치)가 1개만 있는 퍼셉트론을 가져와보자\n",
        "\n",
        "예측값 = w*x + b (선형)\n",
        "\n",
        "(실제값-(wx+b))^2 을 풀어보면 손실함수는 가중치에 대한 손실값이므로 변수는 w이고 이 함수는 1개의 최적해를 갖는 2차함수 형태인 것을 알 수 있다.\n",
        "\n",
        "2. 경사하강법(Gradient Descent):\n",
        "\n",
        "경사하강법은 1차 미분계수를 이용해 함수의 최소값을 찾아가는 방법으로, 함수 값이 낮아지는 방향으로 독립 변수 값을 변형시켜가면서 최종적으로 최소 함수 값을 갖도록 하는 독립 변수 값을 찾는 방법이다.\n",
        "\n",
        "위에서 봤듯, 손실함수의 개형은 1개의 최적해를 갖는 2차 함수의 형태이므로 경사하강법을 사용하여 최소 함수 값을 갖도록 하는 최적해(가중치)를 탐색해야한다.\n",
        "\n",
        "경사하강법은 임의의 가중치를 설정하여, 그 점에서의 기울기를 계산하고 그 기울기를 힌트로 기울기가 0인 지점을 찾아간다.\n",
        "\n",
        "손실함수의 부호를 반전시켜서 최댓값을 찾는다면 경사상승법이 되나 굳이 사용하지 않는다.\n",
        "\n",
        "기울기는 손실함수에서 임의의 가중치에서 시작하며, 기울기가 음수인 경우에는 양의 방향으로 이동하고 기울기가 양수인 경우에는 음의 방향으로 이동하여 극솟값을 찾아간다.\n",
        "\n",
        "여기서 움직이는 기울기(경사)는 가중치에 대하여 편미분한 벡터이고, 이 가중치를 조금씩 움직인다.\n",
        "\n",
        "3. 경사하강법 공식:\n",
        "\n",
        "x_i+1 = x_i - Learning Rate * f(x_i)의 벡터미분\n",
        "\n",
        "학습률(Learning Rate)은 한 번의 학습에서 얼마나 이동할지를 결정한다.\n",
        "\n",
        "기울기는 f의 각 성분의 편미분으로 구성된 열 벡터로 정의한다.\n",
        "\n",
        "즉, 경사하강법 공식은 <u>현재의 위치 x_i에 학습률에 그 위치에서의 기울기만큼을 곱한 값을 뺀만큼 위치를 이동시켜 다음 위치 x_i+1로 이동한다</u>는 의미이다.\n",
        "\n",
        "여기서 학습률과 기울기 곱을 빼는 이유는 현재의 기울기의 반대방향으로 이동하여 극소값에 도달하기 위함이다.\n",
        "\n",
        "4. 학습률(Learning Rate, LR):\n",
        "\n",
        "위 경사하강법의 공식에서 중요한 것은 학습률인데, 이 학습률에 따라 경사 하강법시, 이동하는 수준이 달라지게 된다.\n",
        "\n",
        "즉, '이동거리 = 학습률 X 기울기'로 움직인다.\n",
        "\n",
        "이는 기울기가 낮다면 학습률이 높다고 할지라도 움직이는 거리가 줄어든다는 소리이고, 큰 고랑에 빠진다면 거기서 나오지 못하고 수렴할 수도 있다는 소리이다.\n",
        "\n",
        "학습률이 낮다면 이동하는 거리가 짧으며, 경사하강법 공식에 의해 이동할수록 기울기가 더욱 감소하므로 짧은 이동거리가 더 짧아진다.\n",
        "\n",
        "그로 인해, 학습률이 낮다면 <u>경사 하강법 알고리즘이 수렴하기 위해 반복해야하는 데이터 양이 많아지므로, 학습시간이 늘어나게 된다.</u>\n",
        "\n",
        "학습률이 지나치게 큰 경우, 크게 이동하므로 수렴이 빨리 발생해 학습시간이 적게 걸린다.\n",
        "\n",
        "그러나 너무 크게 이동하므로 전역 최소값(Global minimum)이 있는 영역을 건너뛰어 지역 최소값(Local minimum)에서 수렴할 수도 있다.\n",
        "\n"
      ],
      "metadata": {
        "id": "zX_9fiFNCQ0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 경사하강법의 한계\n",
        "\n",
        "앞서 손실함수를 기반으로 경사하강법의 개형을 그려보았으나 실제로는 저렇게 깔끔한 2차함수 형태를 그리지 않는다.\n",
        "\n",
        "퍼셉트론의 공식이 활성화 함수를 타게 되면, 손실 함수의 모습은 거시적인 관점에서 봤을 때는 최적해를 1개가진 2차함수의 형태를 그리긴 하지만, 그 모습이 울퉁불퉁해져 최적해에 수렴하기 어려워진다.\n",
        "\n",
        "이번에는 경사하강법의 한계점에 대해 하나하나 짚고 넘어가 보려고 한다.\n",
        "\n",
        "1. 데이터가 많아질수록 계산량 증가\n",
        "\n",
        "앞서 경사하강법은 신경망에서 출력되는 예측값(Predict)과 실제값(Label)의 차이인 손실함수(Loss Function)의 값을 최소화하는 것이 목적이다.\n",
        "\n",
        "그러나, 학습용 데이터 셋이 많아진다면 당연히 계산량도 무지막지하게 많아지게 되는데, 그로 인해 학습 속도가 매우 느려지게 된다.\n",
        "\n",
        "기계학습에서는 아주 거대한 Big data가 사용되게 되는데, 이러한 퍼포먼스 문제는 무시할 수 없다.\n",
        "\n",
        "2. Local minimum(Optima)문제\n",
        "\n",
        "앞서 그린 대략적인 손실함수의 개형은 굉장히 매끈했으나, 활성화 함수로 인해 그 모양이 울퉁불퉁해지게 되고, 그로 인해 최적해에 수렴하지 못할 수 있다.\n",
        "\n",
        "경사하강법의 목적은 <u>손실함수에서 랜덤하게 선택한 가중치를 미분하여 나온 결과를 힌트로 해서, 최적해를 찾아가는 것</u>인데 만약에 랜덤하게 선택된 가중치가 Local minimum에 가까이에 있고, Local minimum에 수렴해버리면, 실제 목표인 Global minimum을 찾지 못하는 문제가 발생할 수 있다.\n",
        "\n",
        "만약에 학습률을 너무 크게 설정한다면, Global minimum에 가까운 곳에서 시작한다 할지라도 구간을 뛰어넘어 Local minimum에 수렴할 수도 있다.\n",
        "\n",
        "그러나, 실제로는 모델의 학습이 Local minimum에 빠져 최적의 가중치를 못 찾는 일이 발생할 위험은 그렇게 크지 않다.\n",
        "\n",
        "학습 시에 가중치를 초기화하여 반복하여 최적해를 찾아가므로 Local minimum에서 수렴하여 Loss값이 0 가까이 떨어지지 못한다할지라도, 시작 위치가 가중치에서 Global minimum에 수렴하여 Loss값이 0에 수렴할 수 있다.\n",
        "\n",
        "즉, 모든 초기화된 가중치가 지역 최솟값에 수렴할 수 있는 위치에 존재하지 않는다면 지역 최솟값 문제는 발생하지 않으므로 Local minimum현상의 발생 위험은 그리 크지 않다고 할 수 있다.\n",
        "\n",
        "3. Plateau 문제\n",
        "\n",
        "Local minimum 문제의 예시에서는 손실함수의 모양이 전반적으로 곡선을 그렸으나, 손실함수의 안에는 평탄한 영역이 존재하기도 한다.\n",
        "\n",
        "플래튜라고 불리는 평탄한 영역에서는 학습속도가 매우 느려지며, 느려지다 못해 정지해버릴 위험이 존재한다.\n",
        "\n",
        "경사하강법의 공식을 보면, '현 지점의 기울기 * 학습률'을 통해 다음 가중치를 결정하는데, 평탄한 영역의 기울기는 매우 낮기 때문에 이동거리가 갈수록 줄어들게 되고, 그로 인해 더 이상 학습이 일어나지 않는 가중치 소실(Gradient Vanishing)현상이 발생할 수 있다.\n",
        "\n",
        "이러한 Plateau현상이 발생하면 극솟값에 수렴하지 못해, 학습시간이 매우 길어지고 경사하강법의 랜덤한 가중치에서 현재의 기울기를 힌트로 기울기가 0인 극솟값에 수렴시켜 최적해를 찾는다는 알고리즘이 제대로 작동하지 못하게 된다.\n",
        "\n",
        "4. Zigzag 문제\n",
        "\n",
        "지금까지 경사하강법을 설명할 때, 이해하기 용이하도록 가중치(w)가 1개만 있는 2차원 그래프를 사용했으나, 실제로는 가중치의 수가 매우 많다.\n",
        "\n",
        "이번엔 가중치가 2개(w1,w2)인 3차원 그래프를 등고선으로 그려보자.\n",
        "\n",
        "가중치의 스케일(크기)이 동일하다면 최적해로 바로 찾아갈 수 있으나, 가중치를 모르는 임의의 값이므로 스케일이 동일하리란 보장이 없다.\n",
        "\n",
        "만약에 가중치 스케일이 다르다면(w1이 더 큰 경우), 두 매개변수 w1의 스케일이 w2보다 크다보니 손실함수는 x축 방향 가중치인 w1의 변화에 매우 둔감하고, y축인 w2의 변화에 매우 민감하게 된다.\n",
        "\n",
        "즉, w2의 크기가 w1에 비해 매우 작다보니 w2가 조금만 변해도 손실함수는 크게 변하게 되어 두 매개변수의 변화에 따른 손실함수 변화가 일정하지 않다.\n",
        "\n",
        "위 경우는 매개변수가 2개밖에 존재하지 않았으나 실제로는 그 수가 수백만개에 달할 수 있을 정도로 많기 때문에 이러한 지그재그 현상은 더욱 복잡해지며, 그로 인해 최적해를 찾아가기가 어려워지고, 학습시간 역시 길어지게 된다.\n",
        "\n",
        "지금까지 경사하강법의 문제점에 대해 알아보았는데, 머신러닝에서는 위 문제들을 해결하기 위해 경사하강법을 효율적으로 사용하기 위한 최적화 기법(Optimizer)들이 매우 많다.\n",
        "\n",
        "ex)SGD, Adam, Momentum, Adagrad .."
      ],
      "metadata": {
        "id": "0-Lk7xLeI9-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습 단위\n",
        "\n",
        "이전에는 경사하강법의 한계에 대해서 학습해보았고 이번에는 경사하강법의 실행과정을 살펴보고, 기본 사용방법인 **배치 경사하강법(Batch Gradient Descent)**이 어떤 단점을 가지고 있기에 최적화 기법의 기반이 되는 경사하강법인 **확률적 경사하강법(Stochastic Gradient Descent, SGD)**이 나오게 되었는지를 알아보도록 한다. 먼저 그전에 학습이 일어나는 구조와 학습 단위에 대한 개념을 알아야 한다.\n",
        "\n",
        "1. 학습의 구조\n",
        "\n",
        "학습은 기본적으로 다음과 같은 구조로 움직인다.\n",
        "\n",
        "1) 임의의 매개변수(가중치)를 정한다.\n",
        "\n",
        "2) 선택된 매개변수로 손실 값을 구하고, 손실함수의 기울기(Gradient)를 계산한다.\n",
        "\n",
        "3) 계산된 기울기와 학습률을 이용해 다음 가중치의 위치로 이동하여 파라미터를 업데이트한다.\n",
        "\n",
        "4) 이동된 지점에서 손실함수의 기울기를 계산하고 3번 과정을 다시 실시한다.\n",
        "\n",
        "5) 손실함수의 기울기가 최솟값에 도달하면 파라미터 업데이트를 멈춘다.\n",
        "\n",
        "2. 학습 단위\n",
        "\n",
        "그런데 위 과정을 보다보면 한 가지 의문이 든다.\n",
        "\n",
        "바로 기울기 계산이 엄청 많이 일어난다는 것인데, 우리가 기계를 학습시킬 때 사용하는 빅 데이터는 일반적으로 최소 1000만건 이상을 가리키며 1억, 10억 건 이상 데이터도 심심치 않게 등장한다는 것이다.\n",
        "\n",
        "이렇게 많은 데이터를 한 번에 모델에 태우게 된다면 아무리 좋은 컴퓨터라도 버티지 못할 것이다.\n",
        "\n",
        "한번의 학습에 모든 학습 데이터셋을 사용한다면 여러 문제를 일으킨다.\n",
        "\n",
        "1) 데이터의 크기가 너무 큰 경우, 메모리가 너무 많이 필요해진다.\n",
        "\n",
        "2) 학습 한번에 계산되어야 할 파라미터(가중치)수가 지나치게 많아지므로 계산 시간이 너무 오래 걸린다.\n",
        "\n",
        "-> Epoch / Batch size / Iteration 개념의 등장 !\n",
        "\n",
        "3. Epoch(에포크)\n",
        "\n",
        "훈련 데이터셋에 포함된 모든 데이터들이 한 번씩 모델을 통과한 횟수로, <u>모든 학습 데이터셋을 학습하는 횟수</u>를 의미한다.\n",
        "\n",
        "1 epoch는 전체 학습 데이터셋이 한 신경망에 적용되어 순전파와 역전파를 통해 신경망을 한번 통과했다는 의미가 된다.\n",
        "\n",
        "즉 epoch가 10회라면, 학습 데이터셋 A를 10회 모델에 학습시켰다는 것이다.\n",
        "\n",
        "epoch를 높일수록, 다양한 무작위 가중치로 학습을 해보므로 **적합한 파라미터를 찾을 확률이 높아진다**(손실 값이 내려간다).\n",
        "\n",
        "그러나, 지나치게 epoch를 높이게 되면 그 학습 데이터셋에 **과적합(Overfitting)**되어 다른 데이터에 대해선 제대로 된 예측을 하지 못할 수 있다.\n",
        "\n",
        "우리는 모델을 만들 때 적절한 epoch 값을 설정해야만 underfitting과 overfitting을 방지할 수 있다.\n",
        "\n",
        "epoch 값이 너무 작다면 underfitting이, 너무 크다면 overfitting이 발생할 확률이 높은 것이다.\n",
        "\n",
        "4. Batch size(배치 사이즈)\n",
        "\n",
        "<u>연산 한번에 들어가는 데이터의 크기</u>를 가리킨다.\n",
        "\n",
        "1 Batch size에 해당하는 데이터 셋을 mini batch라고 한다.\n",
        "\n",
        "1회 epoch안에 m개(m>=1)의 mini batch가 들어가게 되며 만약 m=1인 경우, 배치 학습법이라고 한다.\n",
        "\n",
        "배치사이즈가 너무 큰 경우 한 번에 처리해야할 데이터의 양이 많아지므로 학습 속도가 느려지고, 메모리 부족 문제가 발생할 위험이 있다.\n",
        "\n",
        "반대로 배치사이즈가 너무 작은 경우 적은 데이터를 대상으로 가중치를 업데이트하고, 이 업데이트가 자주 발생하므로 훈련이 불안정해진다.\n",
        "\n",
        "5. Iteration(이터레이션)\n",
        "\n",
        "전체 데이터를 모델에 한번 학습시키는데 필요한 배치의 수를 말한다.\n",
        "\n",
        "즉, <u>1 epoch를 마치는데 필요한 파라미터 업데이트 횟수</u>라고 할 수 있다.\n",
        "\n",
        "각 배치마다 파라미터 업데이트가 한 번씩 이루어지므로, Iteration은 '파라미터 업데이트 횟수 = 미니 배치의 수'가 된다."
      ],
      "metadata": {
        "id": "wBOkCf501lM6"
      }
    }
  ]
}